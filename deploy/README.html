<!DOCTYPE html>
<html>
<head>
<title>README.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="deployment">Deployment</h1>
<p>This document will walk you through the concepts/scripts necessary to deploy a trained CNN (as hdf5) as a http endpoint with Docker.</p>
<h2 id="mlops">MLOps</h2>
<p>After obtaining a satisfying model, we save the weights and architecture in on hdf5 file that will be used for inference. The next step (MLOps) is to make this model available. To do this we create a Docker image for real time inference. The expected end result is a container, running on the Edge, infering in real time a specific type of failure for a machine (in our case a fan). There 3 steps to do this:</p>
<ul>
<li>The first is to create the docker image which will encapsulate the code needed to go from an input (an audio recording) to an output (information output: is the fans functionning normally?).</li>
<li>The second is to push the image to a container registry (in our case ACR) in order to make the Docker image available to different platforms (in our case an Edge runtime)</li>
<li>The fubak step is to pull the image to the platform (in our case an Edge runtime) to deploy our model as a service and provide value to the customer.</li>
</ul>
<p>I will not cover this last step, as this is suited to the DevOps/IoT department. However I will detail on how to make integrating with that department seamless and fast.</p>
<p>For our docker image, we need 4 things. A Dockerfile to generate the backend to run the rest of our scripts, the model we trained (we won't go over that as we have done before), a flask server to retrieve requests and send responses and inference scripts to transform the input (in our case an audio recording) within a request into an output (information on whether the sound corresponds to normal functionning of the machine).</p>
<p>The inference scripts take care of several things. It first loads a given audio in its python environment and extracts the MelSpectrogram associated to it to pass to the model. This is encapsulated in our <em>preprocessing.py</em> file:</p>
<p><em>deploy/scripts/preprocessing.py</em></p>
<pre class="hljs"><code><div><span class="hljs-keyword">import</span> librosa
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">extractFeatures</span><span class="hljs-params">(file_name)</span>:</span>

    <span class="hljs-keyword">try</span>:
        audio, sample_rate = librosa.load(file_name, res_type=<span class="hljs-string">'kaiser_fast'</span>) 
        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=<span class="hljs-number">40</span>)
        pad_width = <span class="hljs-number">2</span>
        mfccs = np.pad(mfccs, pad_width=((<span class="hljs-number">0</span>, <span class="hljs-number">0</span>), (<span class="hljs-number">0</span>, pad_width)), mode=<span class="hljs-string">'constant'</span>)

        <span class="hljs-comment"># Clean-up if needed</span>
        <span class="hljs-comment"># for var in ['audio','sample_rate','pad_width','file_name']:</span>
        <span class="hljs-comment">#     del globals()[var]</span>
        <span class="hljs-comment"># del globals()['var']</span>

    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
        print(e,<span class="hljs-string">"\n"</span>,<span class="hljs-string">"Error encountered while parsing file: "</span>, file_name)
        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span> 

    <span class="hljs-keyword">return</span> mfccs
</div></code></pre>
<p>Once the MelSpectrogram is generated for a given .wav by calling the function we defined above, it reshapes the .wav to fit the first layer of our trained CNN. If the audio isn't exactly 10s0ms long, we control for that and either delete the few ms in excess or fill in the missing ms with the last ms of the audio. Taking the trained model as parameter, we predict the class probabilities for the given MelSpectrogram. We then check the highest probability among the 4 and create a string (message) corresponding to what the sound corresponds to. We also create a boolean to check if it's a failure or just normal functionning. All this information is wrapped in a dictionary 'response', that our service will return.</p>
<p><em>deploy/scripts/prediction.py</em></p>
<pre class="hljs"><code><div><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-keyword">from</span> preprocessing <span class="hljs-keyword">import</span> extractFeatures

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span><span class="hljs-params">(wav, model)</span>:</span>
    mfccs = extractFeatures(wav)
    <span class="hljs-keyword">if</span> mfccs.shape[<span class="hljs-number">1</span>] &gt; <span class="hljs-number">433</span>:
        mfccs = mfccs[:,:<span class="hljs-number">433</span>]
    <span class="hljs-keyword">elif</span> mfccs.shape[<span class="hljs-number">1</span>] &lt; <span class="hljs-number">433</span>:
        mfccs = np.concatenate((mfccs,mfccs[:,(mfccs.shape[<span class="hljs-number">1</span>] - (<span class="hljs-number">433</span>-mfccs.shape[<span class="hljs-number">1</span>])):mfccs.shape[<span class="hljs-number">1</span>]]), axis=<span class="hljs-number">1</span>)
    modelInput = mfccs.reshape(<span class="hljs-number">1</span>, <span class="hljs-number">40</span>, <span class="hljs-number">433</span>, <span class="hljs-number">1</span>)
    results = model.predict(modelInput)
    predProbaList = [results[:,<span class="hljs-number">0</span>][<span class="hljs-number">0</span>],results[:,<span class="hljs-number">1</span>][<span class="hljs-number">0</span>],results[:,<span class="hljs-number">2</span>][<span class="hljs-number">0</span>],results[:,<span class="hljs-number">3</span>][<span class="hljs-number">0</span>]]
    problem = np.argmax(results)
    pred = <span class="hljs-literal">False</span>
    <span class="hljs-keyword">if</span> problem == <span class="hljs-number">0</span>:
        detail = [<span class="hljs-string">'Component OK'</span>]
    <span class="hljs-comment"># pred1 = predProbaList[1] &gt;= 0.7</span>
    <span class="hljs-keyword">if</span> problem == <span class="hljs-number">1</span>:
        detail = [<span class="hljs-string">'Component is imbalanced'</span>]
    <span class="hljs-comment"># pred2 = predProbaList[2] &gt;= 0.7</span>
    <span class="hljs-keyword">if</span> problem == <span class="hljs-number">2</span>:
        detail = [<span class="hljs-string">'Component is clogged'</span>]
    <span class="hljs-comment"># pred3 = predProbaList[3] &gt;= 0.7</span>
    <span class="hljs-keyword">if</span> problem == <span class="hljs-number">3</span>:
        detail = [<span class="hljs-string">'Voltage change'</span>]

    <span class="hljs-keyword">if</span> problem <span class="hljs-keyword">in</span> [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>]:
        pred = <span class="hljs-literal">True</span>

    response = {
        <span class="hljs-string">"Anomaly"</span>:bool(pred),
        <span class="hljs-string">"Details"</span>:{
                <span class="hljs-string">"Message"</span>:detail[<span class="hljs-number">0</span>],
                <span class="hljs-string">"Probabilities"</span>:predProbaList  
            }   
        }

    <span class="hljs-comment"># for var in ['mfccs','model','wav','modelInput','results','predProbaList','problem','pred','detail']:</span>
    <span class="hljs-comment">#     del globals()[var]</span>
    <span class="hljs-comment"># del globals()['var']</span>

    <span class="hljs-keyword">return</span> response
</div></code></pre>
<p>The above scripts are what handle the Input to Output part of our service, going from a raw .wav file to precise information on it. These scripts, to be leveraged, need to be connected to an HTTP endpoint. Basically we need a way to call these scripts, in order to get from any .wav the info on whether it's a normal sound or not. To do that, we set up a flask server which will handle this for us.</p>
<p><em>deploy/scripts/server.py</em></p>
<pre class="hljs"><code><div><span class="hljs-keyword">import</span> os
<span class="hljs-keyword">from</span> flask <span class="hljs-keyword">import</span> Flask, jsonify, request, Response

<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">from</span> prediction <span class="hljs-keyword">import</span> predict
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> load_model
<span class="hljs-keyword">import</span> gc

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyEncoder</span><span class="hljs-params">(json.JSONEncoder)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">default</span><span class="hljs-params">(self, obj)</span>:</span>
        <span class="hljs-keyword">if</span> isinstance(obj, np.integer):
            <span class="hljs-keyword">return</span> int(obj)
        <span class="hljs-keyword">elif</span> isinstance(obj, np.floating):
            <span class="hljs-keyword">return</span> float(obj)
        <span class="hljs-keyword">elif</span> isinstance(obj, np.ndarray):
            <span class="hljs-keyword">return</span> obj.tolist()
        <span class="hljs-keyword">else</span>:
            <span class="hljs-keyword">return</span> super(MyEncoder, self).default(obj)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">flask_app</span><span class="hljs-params">()</span>:</span>
    app = Flask(__name__)
    model = load_model(<span class="hljs-string">'models/weightsMulti.hdf5'</span>)
<span class="hljs-meta">    @app.route('/', methods=['GET'])</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">server_is_up</span><span class="hljs-params">()</span>:</span>
        <span class="hljs-comment"># print("success")</span>
        <span class="hljs-keyword">return</span> <span class="hljs-string">'server is up'</span>

<span class="hljs-meta">    @app.route('/inference', methods=['POST'])</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">start</span><span class="hljs-params">()</span>:</span>
        file = request.files[<span class="hljs-string">'file'</span>]
        pred = predict(file, model)
        <span class="hljs-keyword">return</span> Response(json.dumps(pred, cls=MyEncoder), mimetype=<span class="hljs-string">"application/json"</span>)
        <span class="hljs-comment"># return jsonify({"prediction:":pred})</span>
    gc.collect()
    <span class="hljs-keyword">return</span> app

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:
    app = flask_app()
    app.run(debug=<span class="hljs-literal">True</span>, host=<span class="hljs-string">'0.0.0.0'</span>)
</div></code></pre>
<p>Here we simply start by loading the model into our environment and define two simple routes. A first just to check if the server is running and a second '/inference' which will call the model etc. In this second route, it starts by retrieving an audio file from a given request using the part key 'file'. It then calls the predict function from our <em>prediction.py</em> script (which will also call our extractFeatures function from our <em>preprocessing.py</em> script) which will handle the whole Input to Output pipeline on the .wav we retrieved from the request. Once the pipeline is executed, we simply send as a JSON response the information we built in our <em>prediction.py</em> script. There is also a custom encoder for JSON above our server, to ensure the object types of our responses. It is set to listen on 0.0.0.0 so localhost or 0.0.0.0 and by default on port 5000.</p>
<p>With all the above code, we are able to run a flask server with our inference pipeline locally provided we have the adequate Python environment. However, the goal here is to deploy this to the Edge (or another platform) so to do this we use Docker to encapsulate it all.</p>
<p>Our Dockerfile uses a python-slim image (for size optimization purposes), which has Python built-in as well as a series of Linux/C/C++ libraries already installed. We add one library for image processing purposes (libsndfile1) and pip install the required packages for the execution of our model (tf, keras, etc.). We also clear out caches and other unnecessary files. We then set the entrypoint to a shell script (startFlask.sh which is just: <code>python3 server.py</code>) which simply launches our flask server with python to listen to requests and send responses. We also expose on port 5000 to ensure where Docker is &quot;listening&quot;/&quot;talking&quot;.</p>
<p><em>deploy/Dockerfile</em></p>
<pre class="hljs"><code><div><span class="hljs-keyword">FROM</span> python:<span class="hljs-number">3.8</span>-slim as base

<span class="hljs-keyword">FROM</span> base as builder

<span class="hljs-keyword">RUN</span><span class="bash"> mkdir /install</span>
<span class="hljs-keyword">WORKDIR</span><span class="bash"> /install</span>

<span class="hljs-keyword">COPY</span><span class="bash"> requirements.txt /requirements.txt</span>

<span class="hljs-keyword">RUN</span><span class="bash"> pip install --prefix=/install -r /requirements.txt &amp;&amp; \
    rm -r /root/.cache &amp;&amp; \
    rm -rf /src</span>

<span class="hljs-keyword">FROM</span> base

<span class="hljs-keyword">COPY</span><span class="bash"> --from=builder /install /usr/<span class="hljs-built_in">local</span></span>
<span class="hljs-keyword">COPY</span><span class="bash"> scripts /app</span>

<span class="hljs-keyword">RUN</span><span class="bash"> apt-get update -y &amp;&amp; \
    apt-get -y install libsndfile1 </span>

<span class="hljs-keyword">WORKDIR</span><span class="bash"> /app</span>

<span class="hljs-keyword">EXPOSE</span> <span class="hljs-number">5000</span>

<span class="hljs-keyword">CMD</span><span class="bash"> [<span class="hljs-string">"python3"</span> ,<span class="hljs-string">"server.py"</span>]</span>
</div></code></pre>
<p>In the end, our local directory for building our image should resemble this:</p>
<p><img src="../img/dir.JPG" alt="image"></p>
<p>To exectue build we simply do: <code>docker build -t &lt;name&gt;:&lt;version&gt; .</code></p>
<h2 id="testing-locally--deploying-to-azure">Testing locally &amp; deploying to Azure</h2>
<p>Once your build is done, you can run a container of your image like so: <code>Docker run --name=test -p 5010:5000 &lt;name&gt;:&lt;version&gt;</code></p>
<p>A container will thus be running at http://localhost:5010/ (due to port mapping with -p). You can interact with in to way:</p>
<ul>
<li>
<p><em>this will print 'server is up'</em></p>
<pre><code>curl http://localhost:5010/
</code></pre>
</li>
<li>
<p><em>this will send filename.wav to the server and return the detailed response of the model</em></p>
<pre><code>curl -X POST -F &quot;file=@filename.wav&quot; http://localhost:5010/inference
</code></pre>
</li>
</ul>
<p>The above is to test locally, however we wish to deploy this to any platform (or an Edge runtime in our case). To do this, we need to push our image (once we deem it adequate) to a container registry (ACR in our case). To do this, ensure you have created an ACR on your azure portal. Also make sure you have enabled admin user in the access keys menu in Settings to get credentials. Retrieve the login server, username and password to login:</p>
<ul>
<li><em>this will prompt you to fill username and password</em> <code>Docker login acrname.azurecr.io</code></li>
</ul>
<p>Once you are succesfully logged, you need to tag your image with the adress of your ACR:</p>
<p><code>Docker tag &lt;name&gt;:&lt;version&gt; acrname.azurecr.io/&lt;name&gt;:&lt;version&gt;</code></p>
<p>Then simply push with the following command:</p>
<p><code>Docker push acrname.azurecr.io/&lt;name&gt;:&lt;version&gt;</code></p>
<p>Once the push is finished (can take a while depending on your bandwidth and size of image), you should see it in the repositories menu in Services in your ACR.</p>
<p>To check that it works, click on the 3 dots next to the version of your repository and click 'run instance'. This will create an Azure Container Instance of your image. Fill in a name and configure the port to 5000 for example. After the deployment is done, go the resource and in the overview you should have the IP of your instance. To interact with it, use the same <code>curl</code> instructions as above but replacing localhost with the IP of your instance and set the port to 5000. This should ensure it would work on any platform.</p>

</body>
</html>
