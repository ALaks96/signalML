<!DOCTYPE html>
<html>
<head>
<title>README.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="signal-machine-learning">Signal Machine Learning</h1>
<p>This document will walk you through the concepts, ideas and scripts of this project. To reproduce the results you may navigate to the /tuto folder which contains all the necessary code as python script or notebook depending on your preference. The /deploy folder allows you to generate a docker image for real time inference, provided it has a trained model in hdf5 format in the /deploy/scripts/models folder.</p>
<h2 id="1-intro--problem-statement">1. Intro &amp; problem statement</h2>
<p>The goal of this project is to be able to apply Machine Learning on sound recordings from a machine to obtain a model for real time inference on the Edge. Given this context, we have focused on developing a Machine Learning model for sound classification based on the MIMII dataset created by Hitachi.</p>
<p>In their expirement, they recorded the sound of various machines during normal functioning as well as during certain failures. For each type of machine they have recorded the sound of 4 seperate machines using eight microphones, adding three types of white noise independently (-6db, 0db, +6db). The sound recordings were saved in samples of 10s 0ms in .wav format. The .wav files are saved in either a &quot;normal&quot; or &quot;abnormal&quot; folder depending on wether the machine had an issue during recording. We have focused on one type of machine as this is a demo project, specifically on their sound recordings for a fan. They test for 3 types of problems for the fan: Voltage change, Imbalance and Clogging. The total dataset should amount to:</p>
<p>3 (Types of white noise) x 4 (Number of independent machines) x 2 (normal &amp; abnormal folders for .wav files) = 24 subfolders</p>
<p>For more information on their expriment and the data:</p>
<p>https://zenodo.org/record/3384388</p>
<p>Although one could see the direct use of classifying normal &amp; abnormal sounds of a machine, we took it one step further. We've developed a model that could not only identify abnormal sounds but infer what problem specifically is occuring, as we agreed it would be of bigger added value than classic binary classification.</p>
<h2 id="2-azure-ml-studio--data-set-up">2. Azure ML Studio &amp; Data set-up</h2>
<p>The .wav files organized in their respective folders can be downloaded from the link above as .zip archives. The way we handled it, to avoid downloading locally then uploading to the cload, was to download them directly on our compute instance on Azure ML (you also get datacenter bandwidth for faster download). To do this, we simply open a terminal after launching our compute instance on AMLS. From there we either wget or curl the download links from the folders we previously created for each type of white noise added.</p>
<p>links for download : - https://zenodo.org/record/3384388/files/6_dB_fan.zip?download=1 - https://zenodo.org/record/3384388/files/-6_dB_fan.zip?download=1 - https://zenodo.org/record/3384388/files/0_dB_fan.zip?download=1</p>
<pre class="hljs"><code><div><span class="hljs-comment"># install jar because classic unzip doesn't work too well</span>
apt-get update
apt-get install fastjar

<span class="hljs-comment"># create folders for each type of white noise, or do it directly in AMLS</span>
mkdir data66db
mkdir data6db
mkdir data0db

<span class="hljs-comment"># Download zip archives</span>
<span class="hljs-comment"># Before you download file, cd to the adequate folder</span>
curl -L yourLinkHere -o fan.zip 

<span class="hljs-comment"># unzip fan.zip</span>
jar xvf fan.zip

<span class="hljs-comment"># repeat for 3 types of white noise</span>
</div></code></pre>
<p>You should now have 3 folders (data0db, data6db, data66db for each type of white noise) containing subfolders of all the independent machines themselves containing folders of normal and abnormal sounds.</p>
<h2 id="3-pre-processing--feature-extraction">3. Pre-processing &amp; feature extraction</h2>
<p>A common approach to analysis &amp; machine learning applied to audio is to use MelSpectrograms. In short, they are a numerical representation of a signal in both its time &amp; frequency domain. I won't be detailing on the mathematical concepts behind it, but I do reccomend hevily to understand what they are before going on with the project. A good vulgarization can be found here: https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53 In the article you will also find a link to a video from 3Blue1Brown on the Fourier Transform, an essential mathematical tool in the computation of a MelSpectrogram (highly recommended).</p>
<p>What we need to do is to compute the MelSpectrogram for every .wav file we have. To easily do this, we first generate metadata on the paths to each file, with the information on whether the sound is abnormal or not. The below class does exactly that and saves the metadata as .csv:</p>
<p><em>tuto/PreProcessing/metaPipeline.py</em></p>
<pre class="hljs"><code><div><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> glob

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">PipelineMeta</span><span class="hljs-params">()</span>:</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-string">"""
        Initialize class with paths to normal/abnormal .wav files
        """</span>
        self.paths=[
    <span class="hljs-string">"data0db/fan/id_00"</span>,
    <span class="hljs-string">"data0db/fan/id_02"</span>,
    <span class="hljs-string">"data0db/fan/id_04"</span>,
    <span class="hljs-string">"data0db/fan/id_06"</span>,
    <span class="hljs-string">"data6db/fan/id_00"</span>,
    <span class="hljs-string">"data6db/fan/id_02"</span>,
    <span class="hljs-string">"data6db/fan/id_04"</span>,
    <span class="hljs-string">"data6db/fan/id_06"</span>,
    <span class="hljs-string">"data66db/fan/id_00"</span>,
    <span class="hljs-string">"data66db/fan/id_02"</span>,
    <span class="hljs-string">"data66db/fan/id_04"</span>,
    <span class="hljs-string">"data66db/fan/id_06"</span>,
]

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">datasetGenerator</span><span class="hljs-params">(self,
                        targetDir,
                        normalDirName=<span class="hljs-string">"normal"</span>,
                        abnormalDirName=<span class="hljs-string">"abnormal"</span>,
                        ext=<span class="hljs-string">"wav"</span>)</span>:</span>
        <span class="hljs-string">"""
        For a given path, define path to each .wav, noting information on normal/abnormal

        input

            targetDir: path to ind. machine
            normalDirName: name for normal .wav files
            abnormalDirName: name for abnormal .wav files
            ext: extension for audio files

        output 

            normalSet: pandas dataframe contaning file path to all normal .wav audio files of targetDir
            abnormalSet: same as normalSet, but for abnormal .wav
        """</span>
        <span class="hljs-comment"># 01 normal list generate</span>
        normalFiles = sorted(glob.glob(
            os.path.abspath(<span class="hljs-string">"{dir}/{normalDirName}/*.{ext}"</span>.format(dir=targetDir,
                                                                    normalDirName=normalDirName,
                                                                    ext=ext))))
        normalLabels = <span class="hljs-literal">False</span>

        <span class="hljs-comment"># 02 abnormal list generate</span>
        abnormalFiles = sorted(glob.glob(
            os.path.abspath(<span class="hljs-string">"{dir}/{abnormalDirName}/*.{ext}"</span>.format(dir=targetDir,
                                                                    abnormalDirName=abnormalDirName,
                                                                    ext=ext))))
        abnormalLabels = <span class="hljs-literal">True</span>

        normalSet = pd.DataFrame({<span class="hljs-string">"filePath"</span>:normalFiles,<span class="hljs-string">"label"</span>:normalLabels})
        abnormalSet = pd.DataFrame({<span class="hljs-string">"filePath"</span>:abnormalFiles,<span class="hljs-string">"label"</span>:abnormalLabels})
        <span class="hljs-keyword">return</span> normalSet, abnormalSet

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">metaGenerator</span><span class="hljs-params">(self, save=False, metaFileName=<span class="hljs-string">"Meta.csv"</span>)</span>:</span>
        <span class="hljs-string">"""
        Method to loop through all individual independent machines for all 3 types of added white noise

        input

            save: Boolean parameter to save output of method (metadata dataframe) as csv
            metaFileName: name for .csv if save=True

        output

            meta: Dataframe containing filepaths to every .wav in given paths (self.paths from __init__)
        """</span>
        normalSet, abnormalSet = self.datasetGenerator(self.paths[<span class="hljs-number">0</span>])
        meta = normalSet.append(abnormalSet) 
        <span class="hljs-keyword">for</span> path <span class="hljs-keyword">in</span> self.paths[<span class="hljs-number">1</span>:]:
            normalSet, abnormalSet = self.datasetGenerator(path)
            meta = meta.append(normalSet.append(abnormalSet))
        <span class="hljs-keyword">if</span> save:
            meta.to_csv(metaFileName, index=<span class="hljs-number">0</span>)
        <span class="hljs-keyword">return</span> meta

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__str__</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-keyword">return</span> <span class="hljs-string">'class to retrieve filepath and sound type metadata and save as Pandas dataframe'</span>

<span class="hljs-keyword">if</span> __name__==<span class="hljs-string">'__main__'</span>:
    meta=PipelineMeta()
    meta.metaGenerator(save=<span class="hljs-literal">True</span>)
</div></code></pre>
<p>We also provide some script to investigate the characteristics of the .wav files:</p>
<p><em>tuto/PreProcessing/soundCharacteristics.py</em></p>
<pre class="hljs"><code><div><span class="hljs-keyword">import</span> struct
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">from</span> metaPipeline <span class="hljs-keyword">import</span> PipelineMeta

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WavFileHelper</span><span class="hljs-params">(PipelineMeta)</span>:</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, metaFileName=<span class="hljs-string">'Meta.csv'</span>)</span>:</span>
        <span class="hljs-string">"""
        Initialize class, try to load meta.csv containing filepath metadata 
        Upon failure inherit from PipelineMeta() and generate missing metadata

        input

            metaFileName: Name of saved metadata dataframe
        """</span>
        <span class="hljs-keyword">try</span>:
            self.wavMeta = pd.read_csv(metaFileName)
        <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
            print(e, <span class="hljs-string">"\n"</span>, <span class="hljs-string">"meta.csv does not exist, generating it."</span>)
            super().__init__()
            metaObj =  PipelineMeta()
            self.wavMeta = metaObj.metaGenerator(save=<span class="hljs-literal">True</span>)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">FileProperties</span><span class="hljs-params">(self, filename)</span>:</span>
        <span class="hljs-string">"""
        Using struct library extract basic properties of given .wav file. 

        input

            filename: name of .wav to inspect

        output

            numChannels: Number of channels used to record audio
            sampleRate: Sample rate of audio
            bitDepth: Bit depth of audio file
        """</span>
        waveFile = open(filename,<span class="hljs-string">"rb"</span>)
        riff = waveFile.read(<span class="hljs-number">12</span>)
        fmt = waveFile.read(<span class="hljs-number">36</span>)
        numChannelsString = fmt[<span class="hljs-number">10</span>:<span class="hljs-number">12</span>]
        numChannels = struct.unpack(<span class="hljs-string">'&lt;H'</span>, numChannelsString)[<span class="hljs-number">0</span>]
        sampleRateString = fmt[<span class="hljs-number">12</span>:<span class="hljs-number">16</span>]
        sampleRate = struct.unpack(<span class="hljs-string">"&lt;I"</span>,sampleRateString)[<span class="hljs-number">0</span>]
        bitDepthString = fmt[<span class="hljs-number">22</span>:<span class="hljs-number">24</span>]
        bitDepth = struct.unpack(<span class="hljs-string">"&lt;H"</span>,bitDepthString)[<span class="hljs-number">0</span>]
        <span class="hljs-keyword">return</span> (numChannels, sampleRate, bitDepth)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">readFileProperties</span><span class="hljs-params">(self)</span>:</span>           
        audioData = []
        <span class="hljs-keyword">for</span> index, row <span class="hljs-keyword">in</span> self.wavMeta.iterrows():
            fileName=row[<span class="hljs-string">'filePath'</span>]
            data = self.FileProperties(fileName)
            audioData.append(data)
        audioDf = pd.DataFrame(audioData, columns=[<span class="hljs-string">'numChannels'</span>,<span class="hljs-string">'sampleRate'</span>,<span class="hljs-string">'bitDepth'</span>])
        numChannels = audioDf.numChannels.value_counts(normalize=<span class="hljs-literal">True</span>)
        sampleRate = audioDf.sampleRate.value_counts(normalize=<span class="hljs-literal">True</span>)
        bitDepth = audioDf.bitDepth.value_counts(normalize=<span class="hljs-literal">True</span>)
        characteristics = {
            <span class="hljs-string">"number of channels"</span>:numChannels,
            <span class="hljs-string">"sample rate"</span>:sampleRate,
            <span class="hljs-string">"bit depth"</span>:bitDepth
        }
        <span class="hljs-keyword">return</span> characteristics

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__str__</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-keyword">return</span> <span class="hljs-string">'Class to extract .wav properties (sample rate, num channels, bit depth)'</span>

<span class="hljs-keyword">if</span> __name__==<span class="hljs-string">"__main__"</span>:
    audioProp = WavFileHelper()
    audioProp.readFileProperties(metaFileName=<span class="hljs-string">"meta.csv"</span>)
</div></code></pre>
<p>For each filepath we have generated in the metadata, we use the mfcc class from the librosa package, which is a feature to extract a MelSpectrogram from an audio file quite easily. We should end up with a Pandas DataFrame containing:</p>
<ul>
<li>A column 'feature', consisting of numpy arrays of arrays (with our parameters for the MFCC each sould be 40x433)</li>
<li>A column 'classLabel', a boolean indicating a normal sound or not.</li>
<li>A column 'filePath', containing the filepath of a given file as a string to keep track of the machine etc.</li>
<li>A column 'valData', a boolean indicating validation data.</li>
<li>A column 'valData', a boolean indicating wether the data is an augmented version</li>
</ul>
<p>As a later benefit to train/test/val accuracy when fitting our model, we also applied data augmentation. Similar to what is done with images, we modify slightly the input audio we have to generate more samples. In our case, we stretch, roll and more white noise (seperately) to each .wav file, mutliplying the size of our dataset by 4. This is automated with the following class that inherits from WavFileHelper, which itself inherits from PipelineMeta:</p>
<p><em>tuto/FeatureExtraction/melSpecExtraction.py</em></p>
<pre class="hljs"><code><div><span class="hljs-keyword">from</span> PreProcessing.metaPipeline <span class="hljs-keyword">import</span> PipelineMeta
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> librosa

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MelSpectrogram</span><span class="hljs-params">(PipelineMeta)</span>:</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, metaFileName=<span class="hljs-string">'Meta.csv'</span>)</span>:</span>
        <span class="hljs-string">"""
        Initialize class, try to load meta.csv containing filepath metadata 
        Upon failure inherit from PipelineMeta() and generate missing metadata

        input

            metaFileName: Name of saved metadata dataframe
        """</span>
        <span class="hljs-keyword">try</span>:
            self.meta = pd.read_csv(metaFileName)
        <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
            print(e, <span class="hljs-string">"\n"</span>, <span class="hljs-string">"meta.csv does not exist, generating it."</span>)
            super().__init__()
            metaObj =  PipelineMeta()
            self.meta = metaObj.metaGenerator(save=<span class="hljs-literal">True</span>)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dataAugmentation</span><span class="hljs-params">(self, audio)</span>:</span>
        <span class="hljs-string">"""
        Method to augment data by:
            - Adding white noise (at random)
            - Rolling audio
            - Stretching audio

        input

            audio: Extracted audio from .wav

        output

            audioWn: White noise added audio
            audioRoll: Rolled audio
            audioStretch: Stretched audio
        """</span>
        <span class="hljs-comment"># white noise</span>
        wn = np.random.randn(len(audio))
        audioWn = audio + <span class="hljs-number">0.005</span>*wn
        <span class="hljs-comment"># shifting</span>
        audioRoll = np.roll(audio, <span class="hljs-number">1600</span>)
        <span class="hljs-comment"># Stretching</span>
        audioStretch = librosa.effects.time_stretch(audio, rate=<span class="hljs-number">1</span>)

        <span class="hljs-keyword">return</span> audioWn, audioRoll, audioStretch    

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">pad</span><span class="hljs-params">(self, mfccs, padWidth=<span class="hljs-number">2</span>)</span>:</span>
        <span class="hljs-string">"""
        Classic padding for DL

        input

            mfccs: MelSpectrogram of given audio
            padWidth: Width of padding to be applied
        """</span>
        <span class="hljs-keyword">return</span> np.pad(mfccs, pad_width=((<span class="hljs-number">0</span>, <span class="hljs-number">0</span>), (<span class="hljs-number">0</span>, padWidth)), mode=<span class="hljs-string">'constant'</span>)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mfccGenerator</span><span class="hljs-params">(self, filePath, augment=True)</span>:</span>
        <span class="hljs-string">"""
        Method to extract audio and compute MelSpectrogram for given .wav. 

        input

            filePath: Path to .wav
            augment: Boolean parameter, if True apply augment() method.

        output

            if augment:
                mfccs, mfccsWn, mfccsStretch, mfccsRoll: Padded MelSpectrograms
                for original audio &amp; augmented versions
            else:
                mfccs: Padded MelSpectrogram for original audio
        """</span>
        audio, sampleRate = librosa.load(filePath, res_type=<span class="hljs-string">'kaiser_fast'</span>) 
        mfccs = librosa.feature.mfcc(y=audio, sr=sampleRate, n_mfcc=<span class="hljs-number">40</span>)
        <span class="hljs-keyword">if</span> augment:
            Wn, Roll, Stretch = self.dataAugmentation(audio)
            mfccsWn = librosa.feature.mfcc(y=Wn, sr=sampleRate, n_mfcc=<span class="hljs-number">40</span>)
            mfccsRoll = librosa.feature.mfcc(y=Roll, sr=sampleRate, n_mfcc=<span class="hljs-number">40</span>)
            mfccsStretch = librosa.feature.mfcc(y=Stretch, sr=sampleRate, n_mfcc=<span class="hljs-number">40</span>)
            <span class="hljs-keyword">return</span> self.pad(mfccs), self.pad(mfccsWn), self.pad(mfccsStretch), self.pad(mfccsRoll)
        <span class="hljs-keyword">else</span>:
            <span class="hljs-keyword">return</span> self.pad(mfccs)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">save</span><span class="hljs-params">(self, dataFrame, fileName=<span class="hljs-string">"totalAugmentedDf.npy"</span>)</span>:</span>
        <span class="hljs-string">"""
        Method to save adequatly numpy arrays of arrays in pandas Dataframe using 
        numpy binary format. (Pandas built-in .to_csv() will show problems)

        input

            dataFrame: Pandas DataFrame to save
            fileName: name to give to numpy binary containing DataFrame
        """</span>
        temp = dataFrame.copy().to_numpy()
        np.save(fileName, dataFrame, allow_pickle=<span class="hljs-literal">True</span>)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getMfccs</span><span class="hljs-params">(self, augment=True, save=True)</span>:</span>
        <span class="hljs-string">"""
        Method to extract audio and compute MelSpectrograms recursively on all filepaths
        contained in metadata. 

        input

            augment: Boolean parameter to be passed to mfccGenerator method() for augmenting data
            save: Boolean parameter to save resulting pandas DataFrame

        output

            melSpecDf: Pandas DataFrame containing MelSpectrograms, metadata, booleans to 
                        indicate wether data is for model validation and or if data is 
                        an augmented version or not
        """</span>
        features = []
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(self.meta)):
            valData = <span class="hljs-literal">False</span>
            <span class="hljs-keyword">if</span> i%<span class="hljs-number">1000</span> == <span class="hljs-number">0</span>:
                print(i)
            filePath = self.meta.loc[i,<span class="hljs-string">"filePath"</span>]
            <span class="hljs-keyword">if</span> filePath == <span class="hljs-string">"data0db/fan/id_06"</span>:
                valData=<span class="hljs-literal">True</span>
            classLabel = self.meta.loc[i,<span class="hljs-string">"label"</span>]
            <span class="hljs-keyword">if</span> augment:
                mfccsWn, mfccsRoll, mfccsStretch, mfccs = self.mfccGenerator(filePath, augment=augment)
                augmented = <span class="hljs-literal">False</span>
                features.append([mfccs, classLabel, filePath, valData, augmented])
                augmented = <span class="hljs-literal">True</span>
                features.append([mfccsWn, classLabel, filePath, valData, augmented])
                features.append([mfccsRoll, classLabel, filePath, valData, augmented])
                features.append([mfccsStretch, classLabel, filePath, valData, augmented])
            <span class="hljs-keyword">else</span>:
                mfccs = self.mfccGenerator(filePath, augment=augment)
                features.append([mfccs, classLabel, filePath, valData])
        melSpecDf = pd.DataFrame(features, columns=[<span class="hljs-string">'feature'</span>,
                                                    <span class="hljs-string">'classLabel'</span>,
                                                    <span class="hljs-string">'filePath'</span>,
                                                    <span class="hljs-string">'valData'</span>,
                                                    <span class="hljs-string">'augmented'</span>])
        <span class="hljs-keyword">if</span> save:
            self.save(melSpecDf)

        <span class="hljs-keyword">return</span> melSpecDf

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__str__</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-keyword">return</span> <span class="hljs-string">'Class that will load audio data, augment it if paremeter boolean "augment" is set to True, retrieve mfccs for the audio(s), pad them, label them and add meta information (augmented, validation data) as well as save under npy (numpy binary) if parameter boolean "save" is set to True'</span>

<span class="hljs-keyword">if</span> __name__== <span class="hljs-string">"__main__"</span>:
    melSpectrogram = MelSpectrogram()
    melSpectrogram.getMfccs(augment=<span class="hljs-literal">True</span>, save=<span class="hljs-literal">True</span>)
</div></code></pre>
<p>From this point on, we could simply pass it to a CNN and train it to classify normal from abnormal sounds. However, given an interesting insight on the different types of problems they recorded the fan's sound for we could potentially train a model to detect those specific failures. We unfortunately do not have this multi class label, however it is possible to artificially generate it.</p>
<p>Indeed, under the hypothesis that the sound a fan makes when it is malfunctioning differs depending on the type of problem, the MelSpectrograms for these abnormal sounds should differ. If they indeed do differ, we could typically use clustering to seperate them according to that.</p>
<p>Now given the very high dimensionality of the MelSpectrograms we generated, we could face some issues when clustering them. To aleviate this problem, we used PCA to obtain the n most representative eigenvectors of each MelSpectrogram. Looking at the bias-variance trade-off, we easily see that at most two principal components should be considered.</p>
<p><img src="img/PCAvarbias.JPG" alt="image"></p>
<p>Do note that we have applied dimensionality reduction &amp; clustering ONLY on abnormals sounds as a means to generate multiple failure labels for each type of failure as well as ONLY on the non-augmented audios. This is to avoid having different cluster for the same audio simply due to the morphing we apply to it. Once the label is generated for a given .wav, we can simply expand to its augmented versions.</p>
<p>If we plot the data according to their two most representative principal components we obtain the following swarm plot:</p>
<p><img src="img/PCAswarm.jpg" alt="image"></p>
<p>It's hard to detect the different clusters here, however we know from Hitachi's paper that there are roughly 3 types of failures they have recorded for. We thus assume the clustering algorithm to detect 3 clusters. Looking at the elbow method, it seems we coudl maybe even define 4 clusters.</p>
<p><img src="img/nbclusters.jpg" alt="image"></p>
<p>However this method on its own is not accurate and actual knowledge of the number of clusters to expect surpass it.</p>
<p>There are many choices among clustering methods, we have used KMeans here for the sake of simplicity. The resulting algorithm gives us the following groups:</p>
<p><img src="img/clusters.jpg" alt="image"></p>
<p>If we plot for each cluster the distance to centroid for each point we obtain the following distribution plot:</p>
<p><img src="img/disttocentroid.jpg" alt="image"></p>
<p>The distribution is skewed to the left for each cluster, which is exactly what we want. It basically means that overall the distance to the centroid of a cluster for each point within the cluster is generally low and homogenous. As a &quot;verification procedure&quot;, we even downloaded .wav files from each cluster to listen and see if there are any noteable differences. And we can clearly hear a difference of pitch of some sorts across abnormal sounds. If you want to check them out for yousrelf, here is an example .wav for each class:</p>
<ul>
<li>data66db/fan/id_04/normal/00000126.wav:</li>
</ul>
<p><audio controls=""><source src="sample/normal.wav" type="audio/wav"></audio></p>
<ul>
<li>data6db/fan/id_02/abnormal/00000055.wav:</li>
</ul>
<p><audio controls=""><source src="sample/pb1.wav" type="audio/wav"></audio></p>
<ul>
<li>data6db/fan/id_04/abnormal/00000296.wav:</li>
</ul>
<p><audio controls=""><source src="sample/pb2.wav" type="audio/wav"></audio></p>
<ul>
<li>data0db/fan/id_04/abnormal/00000021.wav:</li>
</ul>
<p><audio controls=""><source src="sample/pb2.wav" type="audio/wav"></audio></p>
<p>We then create a label and set it to 0 in the normal sound MelSpectrograms. After that we simply merge them with the MelSpectrograms and multiclass labels of the abnormal sounds to obtain a full mutli class labeled dataset of our audios. All the above is implemented with the below class:</p>
<p><em>tuto/FeatureExtraction/labelGeneration.py</em></p>
<pre class="hljs"><code><div><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">from</span> melSpecExtraction <span class="hljs-keyword">import</span> MelSpectrogram
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns
<span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> KMeans
<span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA
<span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">GenerateLabel</span><span class="hljs-params">(MelSpectrogram)</span>:</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, dfFileName=<span class="hljs-string">'totalAugmentedDf.npy'</span>)</span>:</span>
        <span class="hljs-string">"""
        Initialize class to generate artificial multi class labels 
        Wrapping to total DF integrated

        input

            dfFileName: Name of binary containing previously generated MelSpectrogram DF
        """</span>
        super().__init__()
        <span class="hljs-keyword">try</span>:
            print(<span class="hljs-string">'this may take a while... loading entire dataframe'</span>)
            features = np.load(dfFileName, allow_pickle=<span class="hljs-literal">True</span>).tolist()
            self.melSpecDf = pd.DataFrame(features, columns=[<span class="hljs-string">'feature'</span>, <span class="hljs-string">'classLabel'</span>, <span class="hljs-string">'filePath'</span>, <span class="hljs-string">'valData'</span>, <span class="hljs-string">'augmented'</span>])
            self.melSpecDf[<span class="hljs-string">'featureShape'</span>] = [x.shape <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> self.melSpecDf[<span class="hljs-string">'feature'</span>]] 
            self.melSpecDf = self.melSpecDf[self.melSpecDf[<span class="hljs-string">'featureShape'</span>] == (<span class="hljs-number">40</span>,<span class="hljs-number">433</span>)]
            print(<span class="hljs-string">'done! Moving on'</span>)
        <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
            print(e, <span class="hljs-string">"\n"</span>, <span class="hljs-string">"MelSpectrograms were not saved, will generate/save/load them"</span>)
            self.melSpecDf = self.getMfccs(augment=<span class="hljs-literal">True</span>, save=<span class="hljs-literal">True</span>)
        indexAbnormal = self.melSpecDf[<span class="hljs-string">'filePath'</span>].str.contains(<span class="hljs-string">'abnormal'</span>)
        indexAugmented = self.melSpecDf[<span class="hljs-string">'augmented'</span>] == <span class="hljs-number">0</span>
        self.abnormalMelSpecDf = self.melSpecDf.loc[(indexAbnormal) &amp; (indexAugmented),]    
        self.normalMelSpecDf = self.melSpecDf.loc[(~indexAbnormal),]
        self.augmentedMelSpecDf = self.melSpecDf.loc[(indexAbnormal) &amp; ~(indexAugmented),]

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">formatData</span><span class="hljs-params">(self)</span>:</span> 
        <span class="hljs-string">"""
        Method to flatten MelSpectrograms for PCA

        output

            flattened MelSpectrograms for all abnormal/original .wav files 
        """</span>
        features = self.abnormalMelSpecDf.feature.tolist()
        featuresArray = np.array(features)
        nsamples, nx, ny = featuresArray.shape

        <span class="hljs-keyword">return</span> featuresArray.reshape((nsamples,nx*ny))

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">elbowMethod</span><span class="hljs-params">(self, features)</span>:</span>
        <span class="hljs-string">"""
        Elbow method visualized to determine optimal nb of clusters

        input

            features: Flattened MelSpectrograms

        output

            graph showing intertia per nb of clusters 
        """</span>
        ks = range(<span class="hljs-number">1</span>, <span class="hljs-number">10</span>)
        inertias = []
        <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> ks:
            <span class="hljs-comment"># Create a KMeans instance with k clusters: model</span>
            model = KMeans(n_clusters=k)
            <span class="hljs-comment"># Fit model to samples</span>
            model.fit(features.iloc[:,:<span class="hljs-number">2</span>])         
            <span class="hljs-comment"># Append the inertia to the list of inertias</span>
            inertias.append(model.inertia_)
        plt.plot(ks, inertias, <span class="hljs-string">'-o'</span>, color=<span class="hljs-string">'black'</span>)
        plt.xlabel(<span class="hljs-string">'number of clusters, k'</span>)
        plt.ylabel(<span class="hljs-string">'inertia'</span>)
        plt.xticks(ks)
        plt.show()

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getPCA</span><span class="hljs-params">(self, nComponents=<span class="hljs-number">20</span>)</span>:</span>
        <span class="hljs-string">"""
        Method to apply PCA to flattened MelSpectrograms
        Automatically reshapes each array using formatData method
        Prompts user for nb of components to consider for PCA based
        on var/bias trade off for a given nb of PC

        input

            nComponents: range of PC to consider for choosing var/bias tradeoff

        output

            n principal components for MelSpectrograms
        """</span>
        self.featuresArrayReshaped = self.formatData()
        self.X_std = StandardScaler().fit_transform(self.featuresArrayReshaped)
        pca = PCA(n_components=nComponents)
        pca.fit_transform(self.X_std)
        features = range(pca.n_components_)
        plt.bar(features, pca.explained_variance_ratio_, color=<span class="hljs-string">'black'</span>)
        plt.xlabel(<span class="hljs-string">'PCA features'</span>)
        plt.ylabel(<span class="hljs-string">'variance %'</span>)
        plt.xticks(features)
        plt.show()
        <span class="hljs-keyword">try</span>:
            self.nPC = input(<span class="hljs-string">'From the graph displayed below, how many principal components do you want to keep?'</span>)
            <span class="hljs-keyword">assert</span> isinstance(self.nPC, int), <span class="hljs-string">'Input an integer please'</span>
        <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
            self.nPC = <span class="hljs-number">2</span>
        self.principalComponents = PCA(n_components=self.nPC).fit_transform(self.X_std)
        self.PCA_components = pd.DataFrame(self.principalComponents)
        plt.scatter(self.PCA_components[<span class="hljs-number">0</span>], self.PCA_components[<span class="hljs-number">1</span>], alpha=<span class="hljs-number">.1</span>, color=<span class="hljs-string">'black'</span>)
        plt.xlabel(<span class="hljs-string">'PCA 1'</span>)
        plt.ylabel(<span class="hljs-string">'PCA 2'</span>)
        plt.show()

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">clusterViz</span><span class="hljs-params">(self, reducedData, clusterObj)</span>:</span>
        <span class="hljs-string">"""
        Method for cluster visualization in 2d

        input 

            reducedData: Principal components as array
            clusterObj: Previously initialized cluster object

        output

            2d swarm plot with cluster borders 
        """</span>
        <span class="hljs-comment"># point in the mesh [x_min, x_max]x[y_min, y_max].</span>
        h = <span class="hljs-number">.02</span>     
        <span class="hljs-comment"># Plot the decision boundary. For that, we will assign a color to each</span>
        x_min, x_max = reducedData[:, <span class="hljs-number">0</span>].min() - <span class="hljs-number">1</span>, reducedData[:, <span class="hljs-number">0</span>].max() + <span class="hljs-number">1</span>
        y_min, y_max = reducedData[:, <span class="hljs-number">1</span>].min() - <span class="hljs-number">1</span>, reducedData[:, <span class="hljs-number">1</span>].max() + <span class="hljs-number">1</span>
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
        <span class="hljs-comment"># Obtain labels for each point in mesh. Use last trained model.</span>
        Z = clusterObj.predict(np.c_[xx.ravel(), yy.ravel()])
        Z = Z.reshape(xx.shape)
        plt.figure(<span class="hljs-number">1</span>)
        plt.clf()
        plt.imshow(Z, interpolation=<span class="hljs-string">'nearest'</span>,
                extent=(xx.min(), xx.max(), yy.min(), yy.max()),
                cmap=plt.cm.Paired,
                aspect=<span class="hljs-string">'auto'</span>, origin=<span class="hljs-string">'lower'</span>)

        plt.plot(reducedData[:, <span class="hljs-number">0</span>], reducedData[:, <span class="hljs-number">1</span>], <span class="hljs-string">'k.'</span>, markersize=<span class="hljs-number">2</span>)
        <span class="hljs-comment"># Plot the centroids as a white X</span>
        centroids = clusterObj.cluster_centers_
        plt.scatter(centroids[:, <span class="hljs-number">0</span>], centroids[:, <span class="hljs-number">1</span>],
                    marker=<span class="hljs-string">'x'</span>, s=<span class="hljs-number">169</span>, linewidths=<span class="hljs-number">3</span>,
                    color=<span class="hljs-string">'w'</span>, zorder=<span class="hljs-number">10</span>)
        plt.title(<span class="hljs-string">'K-means clustering melSpectrograms (PCA-reduced data)\n'</span>
                <span class="hljs-string">'Centroids are marked with white cross'</span>)
        plt.xlim(x_min, x_max)
        plt.ylim(y_min, y_max)
        plt.xticks(())
        plt.yticks(())
        plt.show()

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">distToCentroid</span><span class="hljs-params">(self, labels, distances)</span>:</span>
        <span class="hljs-string">"""
        Method to compute and plot distance of each point to the centroïd of its cluster

        input

            labels: list containing cluster label attached to index
            distances: distance from point to centroïd

        output

            distribution plot of distances from each point to its cluster's centroïd
        """</span>
        self.clustersPCA = pd.DataFrame([list(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> zip(labels,distances)],columns=[<span class="hljs-string">'cluster'</span>,<span class="hljs-string">'distance'</span>])
        self.clustersPCA[<span class="hljs-string">'distanceToCluster'</span>] = self.clustersPCA[<span class="hljs-string">'distance'</span>].apply(<span class="hljs-keyword">lambda</span> x: min(x))
        self.clustersPCA[<span class="hljs-string">'distToCluster1'</span>] = self.clustersPCA[<span class="hljs-string">'distance'</span>].apply(<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">0</span>])
        self.clustersPCA[<span class="hljs-string">'distToCluster2'</span>] = self.clustersPCA[<span class="hljs-string">'distance'</span>].apply(<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>])
        self.clustersPCA[<span class="hljs-string">'distToCluster3'</span>] = self.clustersPCA[<span class="hljs-string">'distance'</span>].apply(<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">2</span>])
        self.clustersPCA.cluster.replace({<span class="hljs-number">0</span>:<span class="hljs-number">1</span>, <span class="hljs-number">1</span>:<span class="hljs-number">2</span>, <span class="hljs-number">2</span>:<span class="hljs-number">3</span>}, inplace=<span class="hljs-literal">True</span>)
        sns.displot(data=self.clustersPCA, x=<span class="hljs-string">'distanceToCluster'</span>, hue=<span class="hljs-string">'cluster'</span>, kde=<span class="hljs-literal">True</span>)
        plt.show()

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">viz3d</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-string">"""
        Method to visualize a 3D swarm plot of first 3 PCs, colored by cluster

        output

            3D swarm plot
        """</span>
        pca = PCA(n_components=<span class="hljs-number">3</span>)
        <span class="hljs-keyword">try</span>:
            components = pca.fit_transform(self.X_std)
        <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
            print(e)
            self.getPCA()
        kmeans = KMeans(init=<span class="hljs-string">'k-means++'</span>, n_clusters=self.nClt)
        kmeans.fit(components)
        kmeans.labels_
        total_var = pca.explained_variance_ratio_.sum() * <span class="hljs-number">100</span>
        fig = px.scatter_3d(
            components, x=<span class="hljs-number">0</span>, y=<span class="hljs-number">1</span>, z=<span class="hljs-number">2</span>, color=kmeans.labels_,
            title=<span class="hljs-string">f'Total Explained Variance: <span class="hljs-subst">{total_var:<span class="hljs-number">.2</span>f}</span>%'</span>,
            labels={<span class="hljs-string">'0'</span>: <span class="hljs-string">'PC 1'</span>, <span class="hljs-string">'1'</span>: <span class="hljs-string">'PC 2'</span>, <span class="hljs-string">'2'</span>: <span class="hljs-string">'PC 3'</span>}
        )
        fig.show()

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">cluster</span><span class="hljs-params">(self, checkNbClusters=True, visualize=True, checkDist=True, d3=False)</span>:</span>
        <span class="hljs-string">"""
        Method to apply step by step Dimensionality reduction and clustering on first k chosen PCs
        Calls public methods to help drive PCA &amp; Clustering

        input

            checkNbClusters: Boolean parameter, if True will call elbowMethod visualization method 
            visualize: Boolean parameter, if True will call clusterViz visualization method
            checkDist: Boolean parameter, if True will call distToCentroid visualization method
            d3: Boolean parameter, if True will call viz3d visualization method

        output

            multi class labels for abnormal non augmented .wav files
        """</span>
        self.getPCA(<span class="hljs-number">20</span>)
        <span class="hljs-keyword">if</span> checkNbClusters:
            self.elbowMethod(features=self.PCA_components)
            <span class="hljs-keyword">try</span>:
                self.nClt = input(<span class="hljs-string">'From the graph displayed below, how many clusters do you want?'</span>)
                <span class="hljs-keyword">assert</span> isinstance(self.nClt, int), <span class="hljs-string">'Input an integer please'</span>
            <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
                self.nClt = <span class="hljs-number">3</span>       
        kmeans = KMeans(init=<span class="hljs-string">'k-means++'</span>, n_clusters=self.nClt)
        kmeans.fit(self.principalComponents)
        <span class="hljs-keyword">if</span> visualize <span class="hljs-keyword">and</span> self.nPC==<span class="hljs-number">2</span>:
            self.clusterViz(self.principalComponents, kmeans)
        <span class="hljs-keyword">if</span> checkDist:
            distances = kmeans.fit_transform(self.principalComponents)
            self.distToCentroid(kmeans.labels_, distances)
        <span class="hljs-keyword">if</span> d3:
            self.viz3d()
        self.clustersPCA.reset_index(inplace=<span class="hljs-literal">True</span>, drop=<span class="hljs-literal">True</span>)
        self.abnormalMelSpecDf.reset_index(inplace=<span class="hljs-literal">True</span>, drop=<span class="hljs-literal">True</span>)
        <span class="hljs-keyword">try</span>:
            self.abnormalMelSpecDf[<span class="hljs-string">'cluster'</span>] = self.clustersPCA[<span class="hljs-string">'cluster'</span>]
        <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
            print(e, <span class="hljs-string">'need to generate temp dataframe with clusters'</span>)
            distances = kmeans.fit_transform(self.principalComponents)
            self.distToCentroid(kmeans.labels_, distances)
            self.abnormalMelSpecDf[<span class="hljs-string">'cluster'</span>] = self.clustersPCA[<span class="hljs-string">'cluster'</span>]

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getTotalDf</span><span class="hljs-params">(self, save=True)</span>:</span>
        <span class="hljs-string">"""
        Method to obtain final dataframe with adequate multi class labels

        input

            save: Boolean parameter, if True will save final DF as npy binary

        output

            multiclass labeled MelSpectrogram DF 
        """</span>
        self.cluster()
        self.augmentedMelSpecDf.reset_index(inplace=<span class="hljs-literal">True</span>, drop=<span class="hljs-literal">True</span>)
        self.abnormalMelSpecDf.reset_index(inplace=<span class="hljs-literal">True</span>, drop=<span class="hljs-literal">True</span>)
        self.augmentedMelSpecDf = pd.merge(self.augmentedMelSpecDf,self.abnormalMelSpecDf[[<span class="hljs-string">'filePath'</span>,<span class="hljs-string">'cluster'</span>]], on=<span class="hljs-string">'filePath'</span>, how=<span class="hljs-string">'left'</span>)
        self.normalMelSpecDf[<span class="hljs-string">'cluster'</span>] = <span class="hljs-number">0</span>
        self.normalMelSpecDf.reset_index(inplace=<span class="hljs-literal">True</span>, drop=<span class="hljs-literal">True</span>)
        totalDf = self.normalMelSpecDf.append(self.abnormalMelSpecDf.append(self.augmentedMelSpecDf))
        <span class="hljs-keyword">if</span> save:
            self.save(totalDf, fileName=<span class="hljs-string">"totalAugmentedMultiClassDf.npy"</span>)
        <span class="hljs-keyword">return</span> totalDf

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__str__</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-keyword">return</span> <span class="hljs-string">'Class to apply dimensionality reduction and clustering to generate multiclass label for sound dataset'</span>

<span class="hljs-keyword">if</span> __name__==<span class="hljs-string">"__main__"</span>:
    labelGen = GenerateLabel()
    totalDf = labelGen.getTotalDf(save=<span class="hljs-literal">True</span>)
</div></code></pre>
<p>Visualizations and file saving can be activated by passing adequate boolean parameters upon calling methods.</p>
<h2 id="4-model-fitting">4. Model fitting</h2>
<p>Although Convolutional Neural Networks are the go to solution for the vast majority of the ML community there seems to be a debate on what type of CNN should be used. Approaches using Sparse Encoded CNNs, RNNs, transfer learning with VGG or ImageNet can be seen throughout the net and results are mixed. In the end, testing these different approaches it seems that a classic CNN with a simple architecture seems to yield the best results.</p>
<p>We compile a Keras sequential CNN with 3 recurring blocks of 3 layers:</p>
<ul>
<li>Convolutional layer with kernel size 2 and relu activation function</li>
<li>Max Pooling 2D with pool size 2</li>
<li>dropout with rate 0.2</li>
</ul>
<p>The filter sizes for the convolutional layer in each block double at each sequential block, starting at 16. We set an adam optimizer with a learning rate of 0.01 and set up a GPU Azure compute instance for fast training. After seperating one machine from the rest (as a val set), we split the remaining data as train/test. We then encode the labels and pass them to our CNN for compiling. The above is all implemented in the below class. You may change hyperparemeter settings of CNN. To modify the architecture simply modify the code.</p>
<p><em>tuto/Modelling/CNN.py</em></p>
<pre class="hljs"><code><div><span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Sequential
<span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Dense, Dropout, Activation, Flatten
<span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D
<span class="hljs-keyword">from</span> keras.optimizers <span class="hljs-keyword">import</span> Adam
<span class="hljs-keyword">from</span> keras.utils <span class="hljs-keyword">import</span> np_utils
<span class="hljs-keyword">from</span> keras.utils <span class="hljs-keyword">import</span> to_categorical
<span class="hljs-keyword">from</span> keras.callbacks <span class="hljs-keyword">import</span> ModelCheckpoint 
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics 
<span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> LabelEncoder
<span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split 
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> datetime 

<span class="hljs-keyword">if</span> tf.test.gpu_device_name():
    print(<span class="hljs-string">'Default GPU Device: {}'</span>.format(tf.test.gpu_device_name()))
<span class="hljs-keyword">else</span>:
    print(<span class="hljs-string">"Please install GPU version of TF"</span>)
<span class="hljs-keyword">from</span> FeatureExctraction.labelGeneration <span class="hljs-keyword">import</span> GenerateLabel

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">DCNN</span><span class="hljs-params">(GenerateLabel)</span>:</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, fileName=<span class="hljs-string">'totalAugmentedMultiClassDf.npy'</span>)</span>:</span>
        <span class="hljs-string">"""
        Initialize DCNN class to fit model on previously generated multi class MelSpectrogram DF
        If file is save as npy binary will load, otherwise will generate
        """</span>
        <span class="hljs-keyword">try</span>:
            features = np.load(fileName, allow_pickle=<span class="hljs-literal">True</span>).tolist()
            <span class="hljs-comment"># ~/cloudfiles/code/Users/Alexis.Laks/melspectrogramMutlilabelTrain.npy</span>
            <span class="hljs-comment"># Convert into a Panda dataframe </span>
            self.featuresDf = pd.DataFrame(features, columns=[<span class="hljs-string">'feature'</span>,<span class="hljs-string">'class_label'</span>])
            print(<span class="hljs-string">'Finished feature extraction from '</span>, len(self.featuresdf), <span class="hljs-string">' files'</span>) 
        <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
            print(e, <span class="hljs-string">"Augmented multi labeled melspectrograms weren't generated, will create them and save as npy binary"</span>)
            super().__init__()
            labelGen = GenerateLabel()
            self.featuresDf = labelGen.getTotalDf(save=<span class="hljs-literal">True</span>)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">formatData</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-string">"""
        Method to reshape exogenous variables &amp; encode multi class labels

        output

            Train/test splitted data 
            multi label encoded endogenous variable
        """</span>
        <span class="hljs-comment"># Convert features and corresponding classification labels into numpy arrays</span>
        X = np.array(self.featuresDf.feature.tolist())
        y = np.array(self.featuresDf.cluster.tolist())
        <span class="hljs-comment"># Encode the classification labels</span>
        le = LabelEncoder()
        self.yy = to_categorical(le.fit_transform(y)) 
        <span class="hljs-comment"># split the dataset </span>
        self.xTrain, self.xTest, self.yTrain, self.yTest = train_test_split(X, self.yy, test_size=<span class="hljs-number">0.08</span>, random_state = <span class="hljs-number">42</span>)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">architecture</span><span class="hljs-params">(self, numRows=<span class="hljs-number">40</span>, numColumns=<span class="hljs-number">433</span>, numChannels=<span class="hljs-number">1</span>, filterSize=<span class="hljs-number">2</span>, lr=<span class="hljs-number">0.01</span>)</span>:</span>
        <span class="hljs-string">"""
        Method defining &amp; compiling CNN architecture

        input

            numRows: Number of rows for input array of arrays (MelSpectrogram)
            numColumns: Number of columns for input array of arrays (MelSpectrogram)
            numChannels: Number of channels used to record .wav
            filterSize: Size of filter applied on neural layer
            lr: Learning rate for adam optimizer

        output

            Compiled sequential keras model
        """</span>
        self.formatData()
        self.xTrain = self.xTrain.reshape(self.xTrain.shape[<span class="hljs-number">0</span>], numRows, numColumns, numChannels)
        self.xTest = self.xTest.reshape(self.xTest.shape[<span class="hljs-number">0</span>], numRows, numColumns, numChannels)
        num_labels = self.yy.shape[<span class="hljs-number">1</span>]
        <span class="hljs-comment"># Construct model </span>
        model = Sequential()
        model.add(Conv2D(filters=<span class="hljs-number">16</span>,
                            kernel_size=<span class="hljs-number">2</span>,
                            input_shape=(numRows, numColumns, numChannels),
                            activation=<span class="hljs-string">'relu'</span>))
        model.add(MaxPooling2D(pool_size=<span class="hljs-number">2</span>))
        model.add(Dropout(<span class="hljs-number">0.2</span>))

        model.add(Conv2D(filters=<span class="hljs-number">32</span>,
                        kernel_size=<span class="hljs-number">2</span>, 
                        activation=<span class="hljs-string">'relu'</span>))
        model.add(MaxPooling2D(pool_size=<span class="hljs-number">2</span>))
        model.add(Dropout(<span class="hljs-number">0.2</span>))

        model.add(Conv2D(filters=<span class="hljs-number">64</span>, 
                            kernel_size=<span class="hljs-number">2</span>, 
                            activation=<span class="hljs-string">'relu'</span>))
        model.add(MaxPooling2D(pool_size=<span class="hljs-number">2</span>))
        model.add(Dropout(<span class="hljs-number">0.2</span>))

        model.add(GlobalAveragePooling2D())
        model.add(Dense(num_labels,
                        activation=<span class="hljs-string">'softmax'</span>))

        optimizer = tf.keras.optimizers.Adam(<span class="hljs-number">0.001</span>)
        optimizer.learning_rate.assign(lr)
        <span class="hljs-comment"># Compile the model</span>
        model.compile(loss=<span class="hljs-string">'categorical_crossentropy'</span>,
                        metrics=[<span class="hljs-string">'accuracy'</span>],
                        optimizer=<span class="hljs-string">'adam'</span>)
        <span class="hljs-comment"># Display model architecture summary </span>
        model.summary()
        <span class="hljs-comment"># Calculate pre-training accuracy </span>
        score = model.evaluate(self.xTest, self.yTest, verbose=<span class="hljs-number">1</span>)
        accuracy = <span class="hljs-number">100</span>*score[<span class="hljs-number">1</span>]
        print(<span class="hljs-string">"Pre-training accuracy: %.4f%%"</span> % accuracy) 

        <span class="hljs-keyword">return</span> model

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fit</span><span class="hljs-params">(self, numEpochs=<span class="hljs-number">150</span>, numBatchSize=<span class="hljs-number">256</span>)</span>:</span>
        <span class="hljs-string">"""
        Method to fit compiled model to train/test data

        input

            numEpochs: Number of epochs for training
            numBatchSize: Batch size for neural layer 

        output

            Trained multiclass sequential DCNN, saved to hdf5 @saved_models/weights.best.basic_cnn.hdf5
        """</span>
        model = self.architecture()
        checkpointer = ModelCheckpoint(filepath=<span class="hljs-string">'saved_models/weights.best.basic_cnn.hdf5'</span>, 
                                        verbose=<span class="hljs-number">1</span>, 
                                        save_best_only=<span class="hljs-literal">True</span>)
        start = datetime.now()
        model.fit(self.xTrain,
                    self.yTrain, 
                    batch_size=numBatchSize, 
                    epochs=numEpochs, 
                    validation_data=(self.xTest, self.yTest), 
                    callbacks=[checkpointer],
                    verbose=<span class="hljs-number">1</span>)
        duration = datetime.now() - start
        print(<span class="hljs-string">"Training completed in time: "</span>, duration)
        <span class="hljs-comment"># Evaluating the model on the training and testing set</span>
        score = model.evaluate(self.xTrain,
                                self.yTrain,
                                verbose=<span class="hljs-number">0</span>)
        print(<span class="hljs-string">"Training Accuracy: "</span>, score[<span class="hljs-number">1</span>])
        score = model.evaluate(self.xTest,
                                self.yTest,
                                verbose=<span class="hljs-number">0</span>)
        print(<span class="hljs-string">"Testing Accuracy: "</span>, score[<span class="hljs-number">1</span>])

        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__str__</span><span class="hljs-params">(self)</span>:</span>
            <span class="hljs-keyword">return</span> <span class="hljs-string">'Class to train DCNN on MelSpectrograms, augmented, with mutli label'</span>

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:
    model = DCNN()
    model.fit()
</div></code></pre>
<p>After training on 150 epochs, we obtain a training accuracy of 93% and a test accuracy of 92% which are quite satisfying.</p>
<p>All the above scripts has been formatted into a pipeline that can be executed with main.py. Alternitavely, you could execute the above scripts chunk by chunk in the <code>Notebook/AudioSignalML.ipynb</code> jupyter notebook.</p>
<p>The structure of our repo is the following:</p>
<p><img src="img/dir2.jpg" alt="image"></p>
<h2 id="5-mlops--deployment">5. MLOps &amp; deployment</h2>
<p>After obtaining a satisfying model, we save the weights and architecture in on hdf5 file that will be used for inference. The next step (MLOps) is to make this model available. To do this we create a Docker image for real time inference. The expected end result is a container, running on the Edge, infering in real time a specific type of failure for a machine (in our case a fan). There 3 steps to do this:</p>
<ul>
<li>The first is to create the docker image which will encapsulate the code needed to go from an input (an audio recording) to an output (information output: is the fans functionning normally?).</li>
<li>The second is to push the image to a container registry (in our case ACR) in order to make the Docker image available to different platforms (in our case an Edge runtime)</li>
<li>The fubak step is to pull the image to the platform (in our case an Edge runtime) to deploy our model as a service and provide value to the customer.</li>
</ul>
<p>I will not cover this last step, as this is suited to the DevOps/IoT department. However I will detail on how to make integrating with that department seamless and fast.</p>
<p>For our docker image, we need 4 things. A Dockerfile to generate the backend to run the rest of our scripts, the model we trained (we won't go over that as we have done before), a flask server to retrieve requests and send responses and inference scripts to transform the input (in our case an audio recording) within a request into an output (information on whether the sound corresponds to normal functionning of the machine).</p>
<p>The inference scripts take care of several things. It first loads a given audio in its python environment and extracts the MelSpectrogram associated to it to pass to the model. This is encapsulated in our <em>preprocessing.py</em> file:</p>
<p><em>deploy/scripts/preprocessing.py</em></p>
<pre class="hljs"><code><div><span class="hljs-keyword">import</span> librosa
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">extractFeatures</span><span class="hljs-params">(file_name)</span>:</span>

    <span class="hljs-keyword">try</span>:
        audio, sample_rate = librosa.load(file_name, res_type=<span class="hljs-string">'kaiser_fast'</span>) 
        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=<span class="hljs-number">40</span>)
        pad_width = <span class="hljs-number">2</span>
        mfccs = np.pad(mfccs, pad_width=((<span class="hljs-number">0</span>, <span class="hljs-number">0</span>), (<span class="hljs-number">0</span>, pad_width)), mode=<span class="hljs-string">'constant'</span>)

        <span class="hljs-comment"># Clean-up if needed</span>
        <span class="hljs-comment"># for var in ['audio','sample_rate','pad_width','file_name']:</span>
        <span class="hljs-comment">#     del globals()[var]</span>
        <span class="hljs-comment"># del globals()['var']</span>

    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
        print(e,<span class="hljs-string">"\n"</span>,<span class="hljs-string">"Error encountered while parsing file: "</span>, file_name)
        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span> 

    <span class="hljs-keyword">return</span> mfccs
</div></code></pre>
<p>Once the MelSpectrogram is generated for a given .wav by calling the function we defined above, it reshapes the .wav to fit the first layer of our trained CNN. If the audio isn't exactly 10s0ms long, we control for that and either delete the few ms in excess or fill in the missing ms with the last ms of the audio. Taking the trained model as parameter, we predict the class probabilities for the given MelSpectrogram. We then check the highest probability among the 4 and create a string (message) corresponding to what the sound corresponds to. We also create a boolean to check if it's a failure or just normal functionning. All this information is wrapped in a dictionary 'response', that our service will return.</p>
<p><em>deploy/scripts/prediction.py</em></p>
<pre class="hljs"><code><div><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-keyword">from</span> preprocessing <span class="hljs-keyword">import</span> extractFeatures

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span><span class="hljs-params">(wav, model)</span>:</span>
    mfccs = extractFeatures(wav)
    <span class="hljs-keyword">if</span> mfccs.shape[<span class="hljs-number">1</span>] &gt; <span class="hljs-number">433</span>:
        mfccs = mfccs[:,:<span class="hljs-number">433</span>]
    <span class="hljs-keyword">elif</span> mfccs.shape[<span class="hljs-number">1</span>] &lt; <span class="hljs-number">433</span>:
        mfccs = np.concatenate((mfccs,mfccs[:,(mfccs.shape[<span class="hljs-number">1</span>] - (<span class="hljs-number">433</span>-mfccs.shape[<span class="hljs-number">1</span>])):mfccs.shape[<span class="hljs-number">1</span>]]), axis=<span class="hljs-number">1</span>)
    modelInput = mfccs.reshape(<span class="hljs-number">1</span>, <span class="hljs-number">40</span>, <span class="hljs-number">433</span>, <span class="hljs-number">1</span>)
    results = model.predict(modelInput)
    predProbaList = [results[:,<span class="hljs-number">0</span>][<span class="hljs-number">0</span>],results[:,<span class="hljs-number">1</span>][<span class="hljs-number">0</span>],results[:,<span class="hljs-number">2</span>][<span class="hljs-number">0</span>],results[:,<span class="hljs-number">3</span>][<span class="hljs-number">0</span>]]
    problem = np.argmax(results)
    pred = <span class="hljs-literal">False</span>
    <span class="hljs-keyword">if</span> problem == <span class="hljs-number">0</span>:
        detail = [<span class="hljs-string">'Component OK'</span>]
    <span class="hljs-comment"># pred1 = predProbaList[1] &gt;= 0.7</span>
    <span class="hljs-keyword">if</span> problem == <span class="hljs-number">1</span>:
        detail = [<span class="hljs-string">'Component is imbalanced'</span>]
    <span class="hljs-comment"># pred2 = predProbaList[2] &gt;= 0.7</span>
    <span class="hljs-keyword">if</span> problem == <span class="hljs-number">2</span>:
        detail = [<span class="hljs-string">'Component is clogged'</span>]
    <span class="hljs-comment"># pred3 = predProbaList[3] &gt;= 0.7</span>
    <span class="hljs-keyword">if</span> problem == <span class="hljs-number">3</span>:
        detail = [<span class="hljs-string">'Voltage change'</span>]

    <span class="hljs-keyword">if</span> problem <span class="hljs-keyword">in</span> [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>]:
        pred = <span class="hljs-literal">True</span>

    response = {
        <span class="hljs-string">"Anomaly"</span>:bool(pred),
        <span class="hljs-string">"Details"</span>:{
                <span class="hljs-string">"Message"</span>:detail[<span class="hljs-number">0</span>],
                <span class="hljs-string">"Probabilities"</span>:predProbaList  
            }   
        }

    <span class="hljs-comment"># for var in ['mfccs','model','wav','modelInput','results','predProbaList','problem','pred','detail']:</span>
    <span class="hljs-comment">#     del globals()[var]</span>
    <span class="hljs-comment"># del globals()['var']</span>

    <span class="hljs-keyword">return</span> response
</div></code></pre>
<p>The above scripts are what handle the Input to Output part of our service, going from a raw .wav file to precise information on it. These scripts, to be leveraged, need to be connected to an HTTP endpoint. Basically we need a way to call these scripts, in order to get from any .wav the info on whether it's a normal sound or not. To do that, we set up a flask server which will handle this for us.</p>
<p><em>deploy/scripts/server.py</em></p>
<pre class="hljs"><code><div><span class="hljs-keyword">import</span> os
<span class="hljs-keyword">from</span> flask <span class="hljs-keyword">import</span> Flask, jsonify, request, Response

<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">from</span> prediction <span class="hljs-keyword">import</span> predict
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> load_model
<span class="hljs-keyword">import</span> gc

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyEncoder</span><span class="hljs-params">(json.JSONEncoder)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">default</span><span class="hljs-params">(self, obj)</span>:</span>
        <span class="hljs-keyword">if</span> isinstance(obj, np.integer):
            <span class="hljs-keyword">return</span> int(obj)
        <span class="hljs-keyword">elif</span> isinstance(obj, np.floating):
            <span class="hljs-keyword">return</span> float(obj)
        <span class="hljs-keyword">elif</span> isinstance(obj, np.ndarray):
            <span class="hljs-keyword">return</span> obj.tolist()
        <span class="hljs-keyword">else</span>:
            <span class="hljs-keyword">return</span> super(MyEncoder, self).default(obj)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">flask_app</span><span class="hljs-params">()</span>:</span>
    app = Flask(__name__)
    model = load_model(<span class="hljs-string">'models/weightsMulti.hdf5'</span>)
<span class="hljs-meta">    @app.route('/', methods=['GET'])</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">server_is_up</span><span class="hljs-params">()</span>:</span>
        <span class="hljs-comment"># print("success")</span>
        <span class="hljs-keyword">return</span> <span class="hljs-string">'server is up'</span>

<span class="hljs-meta">    @app.route('/inference', methods=['POST'])</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">start</span><span class="hljs-params">()</span>:</span>
        file = request.files[<span class="hljs-string">'file'</span>]
        pred = predict(file, model)
        <span class="hljs-keyword">return</span> Response(json.dumps(pred, cls=MyEncoder), mimetype=<span class="hljs-string">"application/json"</span>)
        <span class="hljs-comment"># return jsonify({"prediction:":pred})</span>
    gc.collect()
    <span class="hljs-keyword">return</span> app

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:
    app = flask_app()
    app.run(debug=<span class="hljs-literal">True</span>, host=<span class="hljs-string">'0.0.0.0'</span>)
</div></code></pre>
<p>Here we simply start by loading the model into our environment and define two simple routes. A first just to check if the server is running and a second '/inference' which will call the model etc. In this second route, it starts by retrieving an audio file from a given request using the part key 'file'. It then calls the predict function from our <em>prediction.py</em> script (which will also call our extractFeatures function from our <em>preprocessing.py</em> script) which will handle the whole Input to Output pipeline on the .wav we retrieved from the request. Once the pipeline is executed, we simply send as a JSON response the information we built in our <em>prediction.py</em> script. There is also a custom encoder for JSON above our server, to ensure the object types of our responses. It is set to listen on 0.0.0.0 so localhost or 0.0.0.0 and by default on port 5000.</p>
<p>With all the above code, we are able to run a flask server with our inference pipeline locally provided we have the adequate Python environment. However, the goal here is to deploy this to the Edge (or another platform) so to do this we use Docker to encapsulate it all.</p>
<p>Our Dockerfile uses a python-slim image (for size optimization purposes), which has Python built-in as well as a series of Linux/C/C++ libraries already installed. We add one library for image processing purposes (libsndfile1) and pip install the required packages for the execution of our model (tf, keras, etc.). We also clear out caches and other unnecessary files. We then set the entrypoint to a shell script (startFlask.sh which is just: <code>python3 server.py</code>) which simply launches our flask server with python to listen to requests and send responses. We also expose on port 5000 to ensure where Docker is &quot;listening&quot;/&quot;talking&quot;.</p>
<p><em>deploy/Dockerfile</em></p>
<pre class="hljs"><code><div><span class="hljs-keyword">FROM</span> python:<span class="hljs-number">3.8</span>-slim as base

<span class="hljs-keyword">FROM</span> base as builder

<span class="hljs-keyword">RUN</span><span class="bash"> mkdir /install</span>
<span class="hljs-keyword">WORKDIR</span><span class="bash"> /install</span>

<span class="hljs-keyword">COPY</span><span class="bash"> requirements.txt /requirements.txt</span>

<span class="hljs-keyword">RUN</span><span class="bash"> pip install --prefix=/install -r /requirements.txt &amp;&amp; \
    rm -r /root/.cache &amp;&amp; \
    rm -rf /src</span>

<span class="hljs-keyword">FROM</span> base

<span class="hljs-keyword">COPY</span><span class="bash"> --from=builder /install /usr/<span class="hljs-built_in">local</span></span>
<span class="hljs-keyword">COPY</span><span class="bash"> scripts /app</span>

<span class="hljs-keyword">RUN</span><span class="bash"> apt-get update -y &amp;&amp; \
    apt-get -y install libsndfile1 </span>

<span class="hljs-keyword">WORKDIR</span><span class="bash"> /app</span>

<span class="hljs-keyword">EXPOSE</span> <span class="hljs-number">5000</span>

<span class="hljs-keyword">CMD</span><span class="bash"> [<span class="hljs-string">"python3"</span> ,<span class="hljs-string">"server.py"</span>]</span>
</div></code></pre>
<p>In the end, our local directory for building our image should resemble this:</p>
<p><img src="img/dir.JPG" alt="image"></p>
<p>To exectue build we simply do: <code>docker build -t &lt;name&gt;:&lt;version&gt; .</code></p>
<h2 id="6-testing-locally--deploying-to-azure">6. Testing locally &amp; deploying to Azure</h2>
<p>Once your build is done, you can run a container of your image like so: <code>Docker run --name=test -p 5010:5000 &lt;name&gt;:&lt;version&gt;</code></p>
<p>A container will thus be running at http://localhost:5010/ (due to port mapping with -p). You can interact with in to way:</p>
<ul>
<li>
<p><em>this will print 'server is up'</em></p>
<pre><code>curl http://localhost:5010/
</code></pre>
</li>
<li>
<p><em>this will send filename.wav to the server and return the detailed response of the model</em></p>
<pre><code>curl -X POST -F &quot;file=@filename.wav&quot; http://localhost:5010/inference
</code></pre>
</li>
</ul>
<p>The above is to test locally, however we wish to deploy this to any platform (or an Edge runtime in our case). To do this, we need to push our image (once we deem it adequate) to a container registry (ACR in our case). To do this, ensure you have created an ACR on your azure portal. Also make sure you have enabled admin user in the access keys menu in Settings to get credentials. Retrieve the login server, username and password to login:</p>
<ul>
<li><em>this will prompt you to fill username and password</em> <code>Docker login acrname.azurecr.io</code></li>
</ul>
<p>Once you are succesfully logged, you need to tag your image with the adress of your ACR:</p>
<p><code>Docker tag &lt;name&gt;:&lt;version&gt; acrname.azurecr.io/&lt;name&gt;:&lt;version&gt;</code></p>
<p>Then simply push with the following command:</p>
<p><code>Docker push acrname.azurecr.io/&lt;name&gt;:&lt;version&gt;</code></p>
<p>Once the push is finished (can take a while depending on your bandwidth and size of image), you should see it in the repositories menu in Services in your ACR.</p>
<p>To check that it works, click on the 3 dots next to the version of your repository and click 'run instance'. This will create an Azure Container Instance of your image. Fill in a name and configure the port to 5000 for example. After the deployment is done, go the resource and in the overview you should have the IP of your instance. To interact with it, use the same <code>curl</code> instructions as above but replacing localhost with the IP of your instance and set the port to 5000. This should ensure it would work on any platform.</p>

</body>
</html>
